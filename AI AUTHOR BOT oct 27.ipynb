{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Environment Variables\n",
    "load_dotenv()  # Load API keys from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import LangChain Modules\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "from langchain.llms import   \n",
    " OpenAI, HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set Up Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Configure API Keys and Paths\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # Get OpenAI API key\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # Get Hugging Face Hub token\n",
    "SAMPLE_WRITINGS_PATH = \"sample_writings.txt\"  # Path to your text file\n",
    "CHUNK_SIZE = 1000  # Size of text chunks for embedding\n",
    "CHUNK_OVERLAP = 0  # Overlap between text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define Functions\n",
    "\n",
    "def load_sample_writings(file_path: str) -> str:\n",
    "    \"\"\"Load the author's sample writings from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sample_writings = f.read()\n",
    "        logger.info(\"Sample writings loaded successfully.\")\n",
    "        return sample_writings\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading sample writings: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_text(text: str, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    \"\"\"Split text into chunks for embedding.\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    logger.info(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def create_vector_store(texts: list, embedding_model: Optional[str] = \"openai\") -> Chroma:\n",
    "    \"\"\"Create a vector store from text chunks using specified embedding model.\"\"\"\n",
    "    if embedding_model == \"openai\":\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    elif embedding_model == \"huggingface\":\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")   \n",
    "\n",
    "    else:\n",
    "        logger.error(f\"Unsupported embedding model: {embedding_model}\")\n",
    "        raise ValueError(f\"Unsupported embedding model: {embedding_model}\")\n",
    "    \n",
    "    vector_store = Chroma.from_texts(texts, embeddings)\n",
    "    logger.info(\"Vector store created successfully.\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm(model: str = \"openai\"):\n",
    "    \"\"\"Initialize the Language Model based on the specified type.\"\"\"\n",
    "    if model == \"openai\":\n",
    "        return OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "    elif model == \"huggingface\":\n",
    "        # Use a specific model from Hugging Face Hub\n",
    "        return HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.1, \"max_length\":512}, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN) \n",
    "    else:\n",
    "        logger.error(f\"Unsupported LLM model: {model}\")\n",
    "        raise ValueError(f\"Unsupported LLM model: {model}\")\n",
    "\n",
    "def create_retrieval_qa(llm, retriever, memory) -> RetrievalQA:\n",
    "    \"\"\"Create a RetrievalQA chain with memory.\"\"\"\n",
    "\n",
    "    # Use a prompt template for better control over the LLM's output\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]   \n",
    "\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,   \n",
    "  # Optionally return source docs for context\n",
    "        memory = memory, # Add memory to the chain\n",
    "        prompt=PROMPT # Use the prompt template\n",
    "    )\n",
    "    logger.info(\"RetrievalQA chain created successfully.\")\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Main Execution Function\n",
    "def main():\n",
    "    # Load and process sample writings\n",
    "    sample_writings = load_sample_writings(SAMPLE_WRITINGS_PATH)\n",
    "    texts = split_text(sample_writings, CHUNK_SIZE, CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embedding model and create vector store\n",
    "embedding_model = \"openai\"  # or \"huggingface\"\n",
    "docsearch = create_vector_store(texts, embedding_model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose LLM and initialize\n",
    "llm_model = \"huggingface\"  # or \"openai\"\n",
    "llm = initialize_llm(model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='question') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RetrievalQA chain\n",
    "qa = create_retrieval_qa(llm, docsearch.as_retriever(), memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive question loop\n",
    "while True:\n",
    "        query = input(\"Enter your question (or 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            response = qa.run(query)\n",
    "            print(\"AI Author Bot Response:\")\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during QA run: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run the Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
