import os
import logging
from typing import Optional
from dotenv import load_dotenv

# 1. Load Environment Variables
load_dotenv()  # Load API keys from .env file

# 2. Import LangChain Modules
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.llms import OpenAI, HuggingFaceHub
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# 3. Set Up Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 4. Configure API Keys and Paths
#OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # Get OpenAI API key
HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")  # Get Hugging Face Hub token
SAMPLE_WRITINGS_PATH = "sample_writings.txt"  # Path to your text file
CHUNK_SIZE = 1000  # Size of text chunks for embedding
CHUNK_OVERLAP = 0  # Overlap between text chunks

# 5. Define Functions

def load_sample_writings(file_path: str) -> str:
    """Load the author's sample writings from a text file."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            sample_writings = f.read()
        logger.info("Sample writings loaded successfully.")
        return sample_writings
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading sample writings: {e}")
        raise

def split_text(text: str, chunk_size: int, chunk_overlap: int) -> list:
    """Split text into chunks for embedding."""
    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = text_splitter.split_text(text)
    logger.info(f"Text split into {len(chunks)} chunks.")
    return chunks

def create_vector_store(texts: list, embedding_model: Optional[str] = "openai") -> Chroma:
    """Create a vector store from text chunks using specified embedding model."""
    if embedding_model == "openai":
        embeddings = OpenAIEmbeddings()
    elif embedding_model == "huggingface":
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    else:
        logger.error(f"Unsupported embedding model: {embedding_model}")
        raise ValueError(f"Unsupported embedding model: {embedding_model}")
    
    vector_store = Chroma.from_texts(texts, embeddings)
    logger.info("Vector store created successfully.")
    return vector_store

def initialize_llm(model: str = "openai"):
    """Initialize the Language Model based on the specified type."""
    if model == "openai":
        return OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
    elif model == "huggingface":
        # Use a specific model from Hugging Face Hub
        return HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0.1, "max_length":512}, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN) 
    else:
        logger.error(f"Unsupported LLM model: {model}")
        raise ValueError(f"Unsupported LLM model: {model}")

def create_retrieval_qa(llm, retriever, memory) -> RetrievalQA:
    """Create a RetrievalQA chain with memory."""

    # Use a prompt template for better control over the LLM's output
    prompt_template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

    {context}

    Question: {question}
    Helpful Answer:"""
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,  # Optionally return source docs for context
        memory = memory, # Add memory to the chain
        prompt=PROMPT # Use the prompt template
    )
    logger.info("RetrievalQA chain created successfully.")
    return qa_chain

# 6. Main Execution Function
def main():
    # Load and process sample writings
    sample_writings = load_sample_writings(SAMPLE_WRITINGS_PATH)
    texts = split_text(sample_writings, CHUNK_SIZE, CHUNK_OVERLAP)
    
    # Choose embedding model and create vector store
    embedding_model = "openai"  # or "huggingface"
    docsearch = create_vector_store(texts, embedding_model=embedding_model)
    
    # Choose LLM and initialize
    llm_model = "huggingface"  # or "openai"
    llm = initialize_llm(model=llm_model)

    # Initialize memory
    memory = ConversationBufferMemory(memory_key="chat_history", input_key='question') 

    # Create RetrievalQA chain
    qa = create_retrieval_qa(llm, docsearch.as_retriever(), memory)

    # Interactive question loop
    while True:
        query = input("Enter your question (or 'exit' to quit): ")
        if query.lower() == "exit":
            break

        try:
            response = qa.run(query)
            print("AI Author Bot Response:")
            print(response)
        except Exception as e:
            logger.error(f"Error during QA run: {e}")

# 7. Run the Main Function
if __name__ == "__main__":
    main()
