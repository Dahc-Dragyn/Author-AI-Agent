import os
import logging
from typing import Optional, List, Union
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
# Load Langchain
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.llms import OpenAI, HuggingFaceHub
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration: set API keys and default paths
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
SAMPLE_WRITINGS_PATH = "sample_writings.txt"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 0

def load_sample_writings(file_path: str) -> str:
    """
    Load the author's sample writings from a text file.
    
    Args:
        file_path (str): Path to the file containing sample writings.
    
    Returns:
        str: Content of the sample writings file.
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            sample_writings = f.read()
        logger.info("Sample writings loaded successfully.")
        return sample_writings
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading sample writings: {e}")
        raise

def split_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """
    Split text into chunks for embedding.
    
    Args:
        text (str): The text to split.
        chunk_size (int): Size of each text chunk.
        chunk_overlap (int): Overlap between consecutive chunks.
    
    Returns:
        list: List of text chunks.
    """
    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = text_splitter.split_text(text)
    logger.info(f"Text split into {len(chunks)} chunks.")
    return chunks

def create_vector_store(texts: List[str], embedding_model: Optional[str] = "openai") -> Chroma:
    """
    Create a vector store from text chunks using the specified embedding model.
    
    Args:
        texts (List[str]): List of text chunks.
        embedding_model (str): Name of the embedding model to use ("openai" or "huggingface").
    
    Returns:
        Chroma: Vector store with embedded text.
    """
    if embedding_model == "openai":
        embeddings = OpenAIEmbeddings()
    elif embedding_model == "huggingface":
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    else:
        logger.error(f"Unsupported embedding model: {embedding_model}")
        raise ValueError(f"Unsupported embedding model: {embedding_model}")
    
    vector_store = Chroma.from_texts(texts, embeddings)
    logger.info("Vector store created successfully.")
    return vector_store

def initialize_llm(model: str = "openai") -> Union[OpenAI, HuggingFaceHub]:
    """
    Initialize the Language Model based on the specified type.
    
    Args:
        model (str): The LLM model type to initialize ("openai" or "huggingface").
    
    Returns:
        Union[OpenAI, HuggingFaceHub]: The initialized LLM model instance.
    """
    if model == "openai":
        return OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
    elif model == "huggingface":
        return HuggingFaceHub(repo_id="google/flan-t5-xl", model_kwargs={"temperature":0.1, "max_length":512}, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)
    else:
        logger.error(f"Unsupported LLM model: {model}")
        raise ValueError(f"Unsupported LLM model: {model}")

def create_retrieval_qa(llm: Union[OpenAI, HuggingFaceHub], retriever, memory: ConversationBufferMemory) -> RetrievalQA:
    """
    Create a RetrievalQA chain with memory.
    
    Args:
        llm (Union[OpenAI, HuggingFaceHub]): Language model instance.
        retriever: Retriever for accessing relevant documents.
        memory (ConversationBufferMemory): Memory buffer for conversation history.
    
    Returns:
        RetrievalQA: The created RetrievalQA chain.
    """
    prompt_template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

    {context}

    Question: {question}
    Helpful Answer:"""
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        memory=memory,
        prompt=PROMPT
    )
    logger.info("RetrievalQA chain created successfully.")
    return qa_chain

def process_batch_queries(queries: List[str], qa: RetrievalQA) -> List[str]:
    """
    Process a batch of queries and return responses.
    
    Args:
        queries (List[str]): List of queries to process.
        qa (RetrievalQA): The QA chain to use for answering.
    
    Returns:
        List[str]: List of responses from the QA chain.
    """
    responses = []
    for query in queries:
        try:
            response = qa.run(query)
            responses.append(response)
            logger.info(f"Processed query: {query}")
        except Exception as e:
            logger.error(f"Error processing query '{query}': {e}")
            responses.append("Error processing this query.")
    return responses

def main():
    sample_writings = load_sample_writings(SAMPLE_WRITINGS_PATH)
    
    texts = split_text(sample_writings, CHUNK_SIZE, CHUNK_OVERLAP)
    
    embedding_model = "openai"
    docsearch = create_vector_store(texts, embedding_model=embedding_model)
    
    llm_model = "huggingface"
    llm = initialize_llm(model=llm_model)

    memory = ConversationBufferMemory(memory_key="chat_history", input_key='question')
    
    qa = create_retrieval_qa(llm, docsearch.as_retriever(), memory)

    batch_queries = [
        "What inspired this author?",
        "Can you summarize their writing style?",
        "What are key themes in the text?",
    ]

    responses = process_batch_queries(batch_queries, qa)
    for i, response in enumerate(responses):
        print(f"Response to query {i+1}: {response}")

if __name__ == "__main__":
    main()

