{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI, HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration (accessing API keys from environment variables)#free and paid are both \n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "SAMPLE_WRITINGS_PATH = \"sample_writings.txt\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 0\n",
    "\n",
    "def load_sample_writings(file_path: str) -> str:\n",
    "    \"\"\"Load the author's sample writings from a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            sample_writings = f.read()\n",
    "        logger.info(\"Sample writings loaded successfully.\")\n",
    "        return sample_writings\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading sample writings: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_text(text: str, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    \"\"\"Split text into chunks for embedding.\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    logger.info(f\"Text split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def create_vector_store(texts: list, embedding_model: Optional[str] = \"openai\") -> Chroma:\n",
    "    \"\"\"Create a vector store from text chunks using specified embedding model.\"\"\"\n",
    "    if embedding_model == \"openai\":\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "    elif embedding_model == \"huggingface\":\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    else:\n",
    "        logger.error(f\"Unsupported embedding model: {embedding_model}\")\n",
    "        raise ValueError(f\"Unsupported embedding model: {embedding_model}\")\n",
    "    \n",
    "    vector_store = Chroma.from_texts(texts, embeddings)\n",
    "    logger.info(\"Vector store created successfully.\")\n",
    "    return vector_store\n",
    "\n",
    "def initialize_llm(model: str = \"openai\") -> Optional:\n",
    "    \"\"\"Initialize the Language Model based on the specified type.\"\"\"\n",
    "    if model == \"openai\":\n",
    "        return OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "    elif model == \"huggingface\":\n",
    "        # Use a specific model from Hugging Face Hub\n",
    "        return HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0.1, \"max_length\":512}, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN) \n",
    "    else:\n",
    "        logger.error(f\"Unsupported LLM model: {model}\")\n",
    "        raise ValueError(f\"Unsupported LLM model: {model}\")\n",
    "\n",
    "def create_retrieval_qa(llm, retriever, memory) -> RetrievalQA:\n",
    "    \"\"\"Create a RetrievalQA chain with memory.\"\"\"\n",
    "\n",
    "    # Use a prompt template for better control over the LLM's output\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,  # Optionally return source docs for context\n",
    "        memory = memory, # Add memory to the chain\n",
    "        prompt=PROMPT # Use the prompt template\n",
    "    )\n",
    "    logger.info(\"RetrievalQA chain created successfully.\")\n",
    "    return qa_chain\n",
    "\n",
    "def main():\n",
    "    # Load sample writings\n",
    "    sample_writings = load_sample_writings(SAMPLE_WRITINGS_PATH)\n",
    "    \n",
    "    # Split text into chunks\n",
    "    texts = split_text(sample_writings, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    \n",
    "    # Create vector store with desired embedding model\n",
    "    embedding_model = \"openai\"  # Change to \"huggingface\" for free embeddings\n",
    "    docsearch = create_vector_store(texts, embedding_model=embedding_model)\n",
    "    \n",
    "    # Initialize Language Model \n",
    "    llm_model = \"huggingface\"  # Or \"openai\"\n",
    "    llm = initialize_llm(model=llm_model)\n",
    "\n",
    "    # Initialize memory\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='question') \n",
    "\n",
    "    # Create RetrievalQA chain with memory\n",
    "    qa = create_retrieval_qa(llm, docsearch.as_retriever(), memory)\n",
    "\n",
    "    # Start interactive session\n",
    "    while True:\n",
    "        query = input(\"Enter your question (or 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            response = qa.run(query)\n",
    "            print(\"AI Author Bot Response:\")\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during QA run: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
